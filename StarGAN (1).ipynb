{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StarGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ga3gOjD00T1Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Library loading"
      ]
    },
    {
      "metadata": {
        "id": "tgFzcOktgq2N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "# !pip install -q tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bChNKIu-sW71",
        "colab_type": "code",
        "outputId": "b7331747-af3a-4e4d-f8bc-be85b8af061c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np \n",
        "import os\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, ReLU, Conv2DTranspose, LeakyReLU, Layer, ZeroPadding2D\n",
        "from tensorflow.keras.activations import tanh\n",
        "import time\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-alpha0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uUUROxYY0k34",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Auxliary function"
      ]
    },
    {
      "metadata": {
        "id": "tgV7jQcngzPR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def random_target_domain_generation():\n",
        "  ''' target domain generation '''\n",
        "  \n",
        "  target_domain_1 = np.random.uniform(low=0., high=3., size=batch_size).astype(np.int32) # facial expression attributes\n",
        "  target_domain_1 = tf.one_hot(target_domain_1, depth=3)\n",
        "  target_domain_2 = np.random.randint(2, size=(batch_size,2)) # male, young attributes\n",
        "  target_domain =  np.concatenate([target_domain_1, target_domain_2], axis=-1)\n",
        "  return target_domain\n",
        "            \n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  ''' from tensorflow homepage '''\n",
        "  ''' batchnorm 바꿔라 '''\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  \n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()\n",
        "  \n",
        "def input_merge(images, domain):\n",
        "  ''' input image and domain merge'''\n",
        "  batch_size = images.shape[0]\n",
        "  image_size = images.shape[1]\n",
        "  channels = domain.shape[1]\n",
        "  merged = np.zeros([batch_size,image_size,image_size,channels])\n",
        "  for batch in range(batch_size):\n",
        "    temp = tf.broadcast_to(domain[batch], [image_size,image_size,channels])\n",
        "    merged[batch] = temp\n",
        "  merged = tf.concat([images, merged], axis=-1)\n",
        "  return merged  \n",
        "  \n",
        "    \n",
        "class InstanceNormalization(tf.keras.layers.Layer):\n",
        "  '''InstanceNormalization for only 4-rank Tensor (image data)'''\n",
        "\n",
        "  def __init__(self, epsilon=1e-5):\n",
        "    super(InstanceNormalization, self).__init__()\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    shape = tf.TensorShape(input_shape)\n",
        "    param_shape = shape[-1]\n",
        "    self.gamma = self.add_weight(name='gamma',\n",
        "                                 shape=param_shape,\n",
        "                                 initializer='ones',\n",
        "                                 trainable=True)\n",
        "    self.beta = self.add_weight(name='beta',\n",
        "                                shape=param_shape,\n",
        "                                initializer='zeros',\n",
        "                                trainable=True)\n",
        "    super(InstanceNormalization, self).build(input_shape)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    input_shape = inputs.get_shape()\n",
        "    HW = input_shape[1]*input_shape[2] \n",
        "    u_ti = 1/HW*np.sum(inputs, axis=(1,2))   # 2x3  \n",
        "    for _ in range(2):\n",
        "      u_ti = np.stack((u_ti, )*input_shape[1], axis=-1) # 2x3x128x128\n",
        "    u_ti = np.swapaxes(u_ti,1,3) # 2x128x128x3  \n",
        "    var_ti = 1/HW*np.sum((inputs - u_ti), axis=(1,2))**2 # 2x3  \n",
        "    for _ in range(2):\n",
        "      var_ti = np.stack((var_ti, )*input_shape[1], axis=-1) # 2x3x128x128\n",
        "    var_ti = np.swapaxes(var_ti,1,3) # 2x128x128x3                      \n",
        "    y_tijk = (inputs - u_ti) / np.sqrt(var_ti +  self.epsilon)  # 2x128x128x3\n",
        "    return self.gamma * y_tijk + self.beta   \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vFsg66bE2W1N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build Generator & Discriminator"
      ]
    },
    {
      "metadata": {
        "id": "VIDwTWlt2VTI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Downsampling_Part(tf.keras.Model):\n",
        "  ''' Downsampling part of Generator'''\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(Downsampling_Part, self).__init__()\n",
        "    self.conv1 = Conv2D(64, kernel_size = 7, strides = 1, padding = 'valid')\n",
        "    self.conv2 = Conv2D(128, kernel_size = 4, strides = 2, padding = 'valid')\n",
        "    self.conv3 = Conv2D(256, kernel_size = 4, strides = 2, padding = 'valid')\n",
        "    self.zeropadding1 = ZeroPadding2D(3)\n",
        "    self.zeropadding2 = ZeroPadding2D(1)\n",
        "    self.zeropadding3 = ZeroPadding2D(1)\n",
        "    self.instancenormalization1 = InstanceNormalization()\n",
        "    self.instancenormalization2 = InstanceNormalization()\n",
        "    self.instancenormalization3 = InstanceNormalization()\n",
        "    self.activation_ReLU = ReLU()\n",
        "  \n",
        "  def call(self, images, labels):\n",
        "    x = input_merge(images,labels)\n",
        "    x = self.zeropadding1(x)\n",
        "    x = self.conv1(x)\n",
        "    x = self.instancenormalization1(x)\n",
        "    x = self.activation_ReLU(x)\n",
        "    assert x.shape == (2, 128, 128, 64) \n",
        "\n",
        "    x = self.zeropadding2(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.instancenormalization2(x)\n",
        "    x = self.activation_ReLU(x)\n",
        "    assert x.shape == (2, 64, 64, 128) \n",
        "\n",
        "    x = self.zeropadding3(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.instancenormalization3(x)\n",
        "    x = self.activation_ReLU(x)\n",
        "    assert x.shape == (2, 32, 32, 256) \n",
        "    return x\n",
        "  \n",
        "\n",
        "class ResnetIdentityBlock(tf.keras.Model):\n",
        "  ''' ResentIdentityBlock for Residual part of Generator'''\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(ResnetIdentityBlock, self).__init__()\n",
        "    self.conv1 = Conv2D(filters=256, kernel_size=3, strides=1, padding='valid')\n",
        "    self.conv2 = Conv2D(filters=256, kernel_size=3, strides=1, padding='valid')\n",
        "    self.zeropadding = ZeroPadding2D(1)\n",
        "    self.instancenormalization = InstanceNormalization()\n",
        "    self.activation_ReLU = ReLU()\n",
        "\n",
        "  def call(self, input_tensor):\n",
        "    x = self.zeropadding(input_tensor)\n",
        "    x = self.conv1(x)\n",
        "    x = self.instancenormalization(x)\n",
        "    x = self.activation_ReLU(x)\n",
        "    x = self.zeropadding(x)\n",
        "    x = self.conv2(x)\n",
        "    x += input_tensor\n",
        "    x = self.activation_ReLU(x)\n",
        "    return x\n",
        "\n",
        "  \n",
        "class Bottleneck_Part(tf.keras.Model):\n",
        "  ''' Bottleneck part of Generator'''\n",
        "  def __init__(self):\n",
        "    super(Bottleneck_Part, self).__init__()\n",
        "    self.ResnetIdentityBlock = ResnetIdentityBlock()\n",
        "    \n",
        "  def call(self, input_tensor):\n",
        "    x  = self.ResnetIdentityBlock(input_tensor)\n",
        "    x  = self.ResnetIdentityBlock(x)\n",
        "    x  = self.ResnetIdentityBlock(x)\n",
        "    x  = self.ResnetIdentityBlock(x)\n",
        "    x  = self.ResnetIdentityBlock(x)\n",
        "    x  = self.ResnetIdentityBlock(x)       \n",
        "    return x\n",
        "  \n",
        "\n",
        "class Upsampling_Part(tf.keras.Model):\n",
        "  ''' Upsampling part of Generator'''\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(Upsampling_Part, self).__init__()\n",
        "    self.deconv1 = Conv2DTranspose(128, kernel_size = 4, strides = 2, padding = 'same')\n",
        "    self.deconv2 = Conv2DTranspose(64, kernel_size = 4, strides = 2, padding = 'same')\n",
        "    self.conv1 = Conv2D(3, kernel_size = 7, strides = 1, padding = 'same')\n",
        "    self.zeropadding1 = ZeroPadding2D(1)\n",
        "    self.zeropadding2 = ZeroPadding2D(1)\n",
        "    self.zeropadding3 = ZeroPadding2D(3)\n",
        "    self.activation_ReLU = ReLU()\n",
        "    self.instancenormalization1 = InstanceNormalization()\n",
        "    self.instancenormalization2 = InstanceNormalization()\n",
        "\n",
        "    \n",
        "  def call(self, x):\n",
        "#     x = self.zeropadding1(x)\n",
        "    x = self.deconv1(x)\n",
        "    x = self.instancenormalization1(x)\n",
        "    x = self.activation_ReLU(x)\n",
        "    print(x.shape)\n",
        "    assert x.shape == (2, 64, 64, 128) \n",
        "\n",
        "#     x = self.zeropadding2(x)\n",
        "    x = self.deconv2(x)\n",
        "    x = self.instancenormalization2(x)\n",
        "    x = self.activation_ReLU(x)\n",
        "    assert x.shape == (2, 128, 128, 64) \n",
        "\n",
        "#     x = self.zeropadding3(x)\n",
        "    x = self.conv1(x)\n",
        "    x = tanh(x)\n",
        "    assert x.shape == (2, 128, 128, 3) \n",
        "    return x\n",
        "  \n",
        "  \n",
        "class Build_generator(tf.keras.Model):\n",
        "  ''' Building a generator'''\n",
        "  def __init__(self):\n",
        "    super(Build_generator, self).__init__()\n",
        "    self.Downsampling = Downsampling_Part()\n",
        "    self.ResidualBlock = Bottleneck_Part()\n",
        "    self.Upsampling = Upsampling_Part()\n",
        "    \n",
        "  def call(self, images, labels):\n",
        "    x = self.Downsampling(images, labels)\n",
        "    print(\"The shape of the input tensor after downsampling :\", x.shape)\n",
        "    x = self.ResidualBlock(x)\n",
        "    print(\"The shape of the input tensor after Bottleneck :\", x.shape)\n",
        "    x = self.Upsampling(x)\n",
        "    print(\"The shape of the input tensor after upsampling :\", x.shape)\n",
        "    return x\n",
        " \n",
        "\n",
        "class Build_discriminator(tf.keras.Model):\n",
        "  ''' Building a discriminator'''\n",
        "  \n",
        "  def __init__(self, image_size, nd):\n",
        "    super(Build_discriminator, self).__init__()\n",
        "    self.conv1 = Conv2D(64, kernel_size = 4, strides = 2, padding = 'valid')\n",
        "    self.conv2 = Conv2D(128, kernel_size = 4, strides = 2, padding = 'valid')\n",
        "    self.conv3 = Conv2D(256, kernel_size = 4, strides = 2, padding = 'valid')\n",
        "    self.conv4 = Conv2D(512, kernel_size = 4, strides = 2, padding = 'valid')\n",
        "    self.conv5 = Conv2D(1024, kernel_size = 4, strides = 2, padding = 'valid')\n",
        "    self.conv6 = Conv2D(2048, kernel_size = 4, strides = 2, padding = 'valid')    \n",
        "    self.conv7_1 = Conv2D(1, kernel_size = 3, strides = 1)    \n",
        "    self.conv7_2 = Conv2D(nd, kernel_size = int(image_size/64), strides = 1)    \n",
        "    self.zeropadding0 = ZeroPadding2D(0) \n",
        "    self.zeropadding = ZeroPadding2D(1)\n",
        "    self.activation_LeakyReLU = LeakyReLU(alpha=0.01)\n",
        "    \n",
        "  def call(self, x):\n",
        "    x = self.zeropadding(x)\n",
        "    x = self.conv1(x)\n",
        "    x = self.activation_LeakyReLU(x)\n",
        "    assert x.shape == (2, 64, 64, 64) \n",
        "    print(\"The shape of the input tensor after Input layer :\", x.shape)\n",
        "\n",
        "    x = self.zeropadding(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.activation_LeakyReLU(x)\n",
        "    assert x.shape == (2, 32, 32, 128) \n",
        "    \n",
        "    x = self.zeropadding(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.activation_LeakyReLU(x)\n",
        "    assert x.shape == (2, 16, 16, 256) \n",
        "    \n",
        "    x = self.zeropadding(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.activation_LeakyReLU(x)\n",
        "    assert x.shape == (2, 8, 8, 512) \n",
        "\n",
        "    x = self.zeropadding(x)\n",
        "    x = self.conv5(x)\n",
        "    x = self.activation_LeakyReLU(x)\n",
        "    assert x.shape == (2, 4, 4, 1024) \n",
        "\n",
        "    x = self.zeropadding(x)\n",
        "    x = self.conv6(x)\n",
        "    x = self.activation_LeakyReLU(x)\n",
        "    assert x.shape == (2, 2, 2, 2048) \n",
        "    print(\"The shape of the input tensor after Hidden layer :\", x.shape)\n",
        "    \n",
        "    x_src = self.zeropadding(x)\n",
        "    D_src = self.conv7_1(x_src)\n",
        "    D_cls = self.conv7_2(x)\n",
        "    assert D_src.shape == (2, 2, 2, 1)\n",
        "    assert D_cls.shape == (2, 1, 1, 5) \n",
        "    print(\"The shape of the input tensor after Output layer :\", D_src.shape, D_cls.shape)\n",
        "    \n",
        "    return  D_src, D_cls\n",
        "  \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Xrbz8jX2hrf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loss"
      ]
    },
    {
      "metadata": {
        "id": "lwN5a8v-2voy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def adverserial_loss(logits, real=True):\n",
        "  cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)  \n",
        "  if real == True:\n",
        "    loss = cross_entropy(tf.ones_like(logits),logits)\n",
        "  else:\n",
        "    loss = cross_entropy(tf.zeros_like(logits),logits)\n",
        "    return loss\n",
        "\n",
        "def reconstruction_loss(image,rec_image,lambda_rec):\n",
        "  return lambda_rec * np.abs(tf.reduce_mean(image  - rec_image))\n",
        " \n",
        "def domain_cls_loss(domain, logits):\n",
        "  cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)  \n",
        "  return lambda_cls * cross_entropy(domain, logits)\n",
        "\n",
        "## G_loss, D_loss\n",
        "def G_loss(fake_D_src, target_D_cls, target_domain,input_image, reconstructed_image, lambda_cls,lambda_rec):\n",
        "  loss = adverserial_loss(fake_D_src, real=True) + lambda_cls * domain_cls_loss(target_domain, target_D_cls) + lambda_rec * reconstruction_loss(input_image, reconstructed_image)\n",
        "  return loss\n",
        "  \n",
        "def D_loss(real_D_src, src_logits, original_domain, original_D_cls, lambda_cls):\n",
        "  loss = -1 * (adverserial_loss(real_D_src, real=True) + adverserial_loss(fake_D_src, real=False)) + lambda_cls* domain_cls_loss(original_domain,original_D_cls)\n",
        "  return loss\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wUJG_4Lj3GnQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Optimizer"
      ]
    },
    {
      "metadata": {
        "id": "P1QTSUBJ3Ipi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G0mI85Hq3kaK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model checkpoint"
      ]
    },
    {
      "metadata": {
        "id": "Syln7SGr3N_3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train loop "
      ]
    },
    {
      "metadata": {
        "id": "GghvR-mU2mKa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_step(input_image, original_domain, target_domain):\n",
        "# '''fake_image (image) # generator: generated fake images\n",
        "#  fake_D_src (logits) # discriminator: real / fake image classification for fake image\n",
        "#  target_D_cls (logits) # discriminator: original / target label classification for fake image\n",
        "#  real_D_src (logits) # discriminator: real / fake image classificiaion for real image\n",
        "#  original_D_cls (logits) # discriminator: original / target label classificiaion for real image\n",
        "#  fake_D_src (logits) # discriminator: original/target classification for fake image\n",
        "#  reconstructed_image (image) # generator: generated real (reconstructed) images '''\n",
        "\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        \n",
        "    # generator + discriminator combined\n",
        "    fake_image = generator(input_image, target_domain)  # step(b)\n",
        "    fake_D_src, target_D_cls = discriminator(fake_image)  # step(d) # 우선 fake image를 넣어서 보조 classification을 학습\n",
        "    reconstructed_image = generator(fake_image, original_domain) # step(c)\n",
        "\n",
        "    # discriminator\n",
        "    real_D_src, original_D_cls = discriminator(input_image) #step(a) \n",
        "    fake_D_src, fake_D_cls = discriminator(fake_image) #step(a)  # 여기서는 보조 classification 학습 안함\n",
        "    \n",
        "    generator_loss = G_loss(fake_D_src, target_D_cls, target_domain,input_image, reconstructed_image, lambda_cls,lambda_rec)\n",
        "    discriminator_loss = D_loss(real_D_src, src_logits, original_domain, original_D_cls, lambda_cls)\n",
        "    \n",
        "    gradients_of_generator = gen_tape.gradient(generator_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(discriminator_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    \n",
        "def train(train_dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for input_image, original_domain in train_dataset:\n",
        "            \n",
        "      target_domain = random_target_domain_generation()\n",
        "      train_step(input_image, original_domain, target_domain)\n",
        "\n",
        "    # Produce images for the GIF as we go\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator,\n",
        "                             epoch + 1,\n",
        "                             seed)\n",
        "\n",
        "    # Save the model every 15 epochs\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  display.clear_output(wait=True)\n",
        "  generate_and_save_images(generator,\n",
        "                           epochs,\n",
        "                           seed)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-g0ruXM93URR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Execution"
      ]
    },
    {
      "metadata": {
        "id": "7CdIzxF-237A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### original data generation\n",
        "N = 10\n",
        "batch_size = 2\n",
        "image_size = 128\n",
        "image_data = np.random.normal(size=[N, image_size, image_size, 3])\n",
        "domain_1 = np.random.uniform(low=0., high=3., size=N).astype(np.int32) # facial expression attributes\n",
        "domain_1 = tf.one_hot(domain_1, depth=3)\n",
        "domain_2 = np.random.randint(2, size=(N,2)) # male, young attributes\n",
        "domain =  np.concatenate([domain_1, domain_2], axis=-1)\n",
        "domain = domain.reshape((-1,1,1, domain.shape[1]))\n",
        "nd = domain.shape[-1] \n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((image_data,domain))\n",
        "train_dataset = train_dataset.batch(batch_size) \n",
        "\n",
        "### Build generator & discriminator\n",
        "generator = Build_generator()\n",
        "discriminator = Build_discriminator(image_size, nd)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "### Loop information\n",
        "epochs = 1\n",
        "num_examples_to_generate = 16\n",
        "seed = tf.random.normal([num_examples_to_generate])\n",
        "lambda_cls = 0.1\n",
        "lambda_rec = 0.5\n",
        "\n",
        "# ### Train\n",
        "# train(train_dataset, epochs)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a5StoDw2A1zQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "0734aeaa-47f2-4a63-e185-9605a2cf6aed"
      },
      "cell_type": "code",
      "source": [
        "# for input_image, original_domain in train_dataset.take(1):    \n",
        "#   target_domain = random_target_domain_generation()\n",
        "# #   train_step(input_image, original_domain, target_domain)\n",
        "#   fake_image = generator(input_image, target_domain)  # step(b)\n",
        "#   fake_D_src, target_D_cls = discriminator(fake_image)  # step(d) # 우선 fake image를 넣어서 보조 classification을 학습\n",
        "#   print(fake_D_src, target_D_cls)\n",
        "#   reconstructed_image = generator(fake_image, original_domain) # step(c)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of the input tensor after downsampling : (2, 32, 32, 256)\n",
            "The shape of the input tensor after Bottleneck : (2, 32, 32, 256)\n",
            "(2, 64, 64, 128)\n",
            "The shape of the input tensor after upsampling : (2, 128, 128, 3)\n",
            "The shape of the input tensor after Input layer : (2, 64, 64, 64)\n",
            "The shape of the input tensor after Hidden layer : (2, 2, 2, 2048)\n",
            "The shape of the input tensor after Output layer : "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vdYx78eLLyYe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "3c7e2c66-fc24-4529-df3f-804e395d535a"
      },
      "cell_type": "code",
      "source": [
        "# a,b = discriminator(fake_image)\n",
        "# a.shape\n",
        "# b.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of the input tensor after Input layer : (2, 64, 64, 64)\n",
            "The shape of the input tensor after Hidden layer : (2, 2, 2, 2048)\n",
            "The shape of the input tensor after Output layer : (2, 2, 2, 1) (2, 1, 1, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}