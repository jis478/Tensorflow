# -*- coding: utf-8 -*-
"""model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QiouZhKG6Ezs6JtHcETVuZP9Xs7xVJIY
"""

class ResnetGenerator(tf.keras.Model):
    def __init__(self, input_nc, output_nc, ngf=64, n_blocks=6):
        assert(n_blocks >= 0)
        super(ResnetGenerator, self).__init__()
        self.output_nc = output_nc
        self.ngf = ngf
        self.n_blocks = n_blocks
        
        DownBlock=tf.keras.Sequential()
        DownBlock.add(Conv(filters = self.ngf, kernel_size = 7, strides = 1, pad = 3, normal = 'IN', act = 'relu', pad_type='REFLECT')

       # Down-Sampling
        n_downsampling = 2
        for i in range(n_downsampling):
            mult = 2**i
            DownBlock.add(Conv(filters = self.ngf*mult*2, kernel_size = 3, strides = 2, pad = 1, normal = 'IN', act = 'relu', pad_type='REFLECT'))

        # Down-Sampling Bottleneck
        mult = 2**n_downsampling
        for i in range(self.n_blocks):
            DownBlock.add(ResnetBlock(self.ngf * mult, use_biase=False))

        # CAM
        self.gap_fc = tf.keras.layers.Dense(1, use_bias=False)
        self.gap = tf.keras.layers.GlobalAveragePooling2D()
        self.gap_fc = tf.keras.layers.Dense(1, use_bias=False)
        self.gmp = tf.keras.layers.GlobalMaxPool2D()
        self.conv1x1 = tf.keras.layers.Conv2D(filters= self.ngf * mult , kernel_size=(1,1), strides=(1,1), use_bias=True))
        self.relu = tf.keras.layers.ReLU()

        # Gamma, Beta block (self.light is not used in Tensorflow as the input dims are automatically detected)
        FC = tf.keras.Sequential()          
        FC.add(tf.keras.layers.Dense(self.ngf * mult, use_bias=False))
        FC.add(tf.keras.layers.ReLU())
        FC.add(tf.keras.layers.Dense(self.ngf * mult, use_bias=False))
        FC.add(tf.keras.layers.ReLU())
        self.gamma = tf.keras.layers.Dense(self.ngf * mult, use_bias=False)  
        self.beta = tf.keras.layers.Dense(self.ngf * mult, use_bias=False)  

        ''' Decoder to be implemented '''


class Conv(tf.keras.layers.Layer):
     def __init__(self, filters, kernel_size, strides, pad, normal, act, use_bias, pad_type='REFLECT'):
       super(Conv, self).__init__()
       self.filters = filters
       self.kernel_size = kernel_size
       self.strides = strides
       self.pad = pad
       self.normal = normal
       self.act = act
       self.use_bias = use_bias
       self.pad_type = pad_type
       
       # normalization
       if self.normal = 'IN':
         self.normal = tfa.layers.InstanceNormalization(axis=3, center=True, scale=True, beta_initializer="random_uniform", gamma_initializer="random_uniform")
       elif self.normal == 'AdaILN':
         ''' to be implemented '''
         pass
       elif self.normal == 'LIN'
         ''' to be implemented '''
         pass
       else:
         self.normal = None     

      # activation
       if self.act == 'relu':
         self.act = tf.keras.layers.ReLU()
       elif self.act == 'tanh'
         self.act = tf.keras.activations.tanh
       else:
         self.act = None

      # conv2d   
       self.conv = tf.keras.layers.Conv2D(filters=self.filters , kernel_size=(self.kernel_size, self.kernel_size), strides=(self.strides, self.strides), use_bias=self.use_bias))

     def __call__(self, x):
         if self.pad > 0:
           x = tf.pad(x, [[0, 0], [self.pad, self.pad], [self.pad, self.pad], [0, 0]], mode=self.pad_type)      
         x = self.conv(x)
         if self.normal is not None: 
           x = self.normal(x)  
         if self.act is not None:
           x = self.act(x)
         return x


class ResnetBlock(tf.keras.layers.Layer): 
  def __init__(self, dim, use_bias):
        super(ResNetBlock, self).__init__()
        self.use_bias = use_bias
        self.dim = dim
        self.conv1 = Conv(filters = self.dim, kernel_size = 3, strides = 1, pad = 0, normal = 'IN', act = 'relu', use_bias=self.use_bias, pad_type=None)
        self.conv2 = Conv(filters = self.dim, kernel_size = 3, strides = 1, pad = 0, normal = 'IN', act = None, use_bias=self.use_bias, pad_type=None)

    def call(self, inputs):
        x = self.conv1(inputs)
        x = self.conv2(x)
        x += inputs
        return x