{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled24.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6BADlutBK2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rho clipper: model안에서 존재하는지, weight가갱신되는지 확인 할 것\n",
        "# summary (Tensorboard)\n",
        "\n",
        "# tf.model 안에서 sequential을 다시 쓰는 것은 튜토리얼에 없음. block의 경우 layer를 쓰라고 나와있음. model은 seralization 및 training 등에 사용\n",
        "#  (training이란.. 모델 container는 안에 있는 var들을 모두 모아서..이건 layer도 되는데? training 가능?)\n",
        "# graph가 연결이 안되어있는 경우 grad를 구하면 None 됨. \n",
        "# self.weight_decay 다 적용 안됨 \n",
        "# Spectral normalization을 dense에도 적용 필요 함 "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OexWnKlVauIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# https://github.com/thisisiron/spectral_normalization-tf2/blob/master/sn.py\n",
        "# https://groups.google.com/a/tensorflow.org/g/discuss/c/PRjyj6tiQvU?pli=1\n",
        "\n",
        "class SpectralNormalization(tf.keras.layers.Wrapper):\n",
        "    def __init__(self, layer, iteration=1, eps=1e-12, training=True, **kwargs):\n",
        "        self.iteration = iteration\n",
        "        self.eps = eps\n",
        "        self.do_power_iteration = training\n",
        "        if not isinstance(layer, tf.keras.layers.Layer):\n",
        "            raise ValueError(\n",
        "                'Please initialize `TimeDistributed` layer with a '\n",
        "                '`Layer` instance. You passed: {input}'.format(input=layer))\n",
        "        super(SpectralNormalization, self).__init__(layer, **kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.layer.build(input_shape)\n",
        "\n",
        "        self.w = self.layer.kernel\n",
        "        self.w_shape = self.w.shape.as_list()\n",
        "        # print('w_shape', self.w_shape)\n",
        "\n",
        "        # 4 x 4 x 3 x 32 (h w previous_c next_c)   # 324 X 1 ( prev_unit, next_unit)\n",
        "        if len(self.w_shape) >=3 : # Conv2D \n",
        "          self.v = self.add_weight(shape=(1, self.w_shape[0] * self.w_shape[1] * self.w_shape[2]),\n",
        "                                  initializer=tf.initializers.TruncatedNormal(stddev=0.02),\n",
        "                                  trainable=False,\n",
        "                                  name='sn_v',\n",
        "                                  dtype=tf.float32)\n",
        "        else:\n",
        "          self.v = self.add_weight(shape=(1, self.w_shape[0]), # 1 x 2048\n",
        "                                  initializer=tf.initializers.TruncatedNormal(stddev=0.02),\n",
        "                                  trainable=False,\n",
        "                                  name='sn_v',\n",
        "                                  dtype=tf.float32)\n",
        "\n",
        "        self.u = self.add_weight(shape=(1, self.w_shape[-1]),\n",
        "                                 initializer=tf.initializers.TruncatedNormal(stddev=0.02),\n",
        "                                 trainable=False,\n",
        "                                 name='sn_u',\n",
        "                                 dtype=tf.float32)\n",
        "\n",
        "        super(SpectralNormalization, self).build()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        self.update_weights()\n",
        "        output = self.layer(inputs)\n",
        "        # self.restore_weights()  # Restore weights because of this formula \"W = W - alpha * W_SN`\"\n",
        "        return output\n",
        "    \n",
        "    def update_weights(self):\n",
        "        w_reshaped = tf.reshape(self.w, [-1, self.w_shape[-1]])\n",
        "        \n",
        "        u_hat = self.u\n",
        "        v_hat = self.v\n",
        "\n",
        "        if self.do_power_iteration:\n",
        "            for _ in range(self.iteration):\n",
        "                v_ = tf.matmul(u_hat, tf.transpose(w_reshaped)) # \n",
        "                v_hat = v_ / (tf.reduce_sum(v_**2)**0.5 + self.eps)\n",
        "\n",
        "                u_ = tf.matmul(v_hat, w_reshaped)\n",
        "                u_hat = u_ / (tf.reduce_sum(u_**2)**0.5 + self.eps)\n",
        "\n",
        "        sigma = tf.matmul(tf.matmul(v_hat, w_reshaped), tf.transpose(u_hat))\n",
        "        self.u.assign(u_hat)\n",
        "        self.v.assign(v_hat)\n",
        "        self.layer.kernel.assign(self.w / sigma)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjKuDHiFO8DS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf \n",
        "import tensorflow_addons as tfa\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ResnetGenerator(tf.keras.Model): #https://www.tensorflow.org/api_docs/python/tf/keras/Sequential (input_shape issue)\n",
        "    def __init__(self, output_nc, ngf=64, n_blocks=6, light=False):\n",
        "        assert(n_blocks >= 0)\n",
        "        super(ResnetGenerator, self).__init__()\n",
        "        self.output_nc = output_nc\n",
        "        self.ngf = ngf\n",
        "        self.n_blocks = n_blocks\n",
        "        self.light = light\n",
        "       \n",
        "        self.DownBlock= []\n",
        "        self.DownBlock.append(Conv(filters = self.ngf, kernel_size = 7, strides = 1, pad = 3, normal = 'IN', \n",
        "                                act = 'relu', use_bias=True, pad_type='REFLECT'))\n",
        "\n",
        "       # Down-Sampling\n",
        "        n_downsampling = 2\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2**i\n",
        "            self.DownBlock.append(Conv(filters = self.ngf*mult*2, kernel_size = 3, strides = 2, pad = 1, normal = 'IN', \n",
        "                                act = 'relu', use_bias=False, pad_type='REFLECT'))\n",
        "\n",
        "        # Down-Sampling Bottleneck\n",
        "        mult = 2**n_downsampling\n",
        "        for i in range(self.n_blocks):\n",
        "           self.DownBlock.append(ResnetBlock(self.ngf * mult, use_bias=False))\n",
        "\n",
        "        # CAM\n",
        "        self.cam_fc = tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.L2(0.0001), kernel_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02), use_bias=True)\n",
        "        self.gap = tf.keras.layers.GlobalAveragePooling2D()\n",
        "        self.gmp = tf.keras.layers.GlobalMaxPool2D()\n",
        "        self.conv1x1 = tf.keras.layers.Conv2D(filters= self.ngf * mult , kernel_size=(1,1), \n",
        "                                              kernel_regularizer=tf.keras.regularizers.L2(0.0001), \n",
        "                                              strides=(1,1), use_bias=True)\n",
        "        self.relu = tf.keras.layers.ReLU()\n",
        "\n",
        "        # Gamma, Beta block (the argument 'self.light' is not used in Tensorflow as the input dims are automatically detected)\n",
        "        self.FC = []\n",
        "        for i in range(2):\n",
        "          self.FC.append(tf.keras.layers.Dense(self.ngf * self.n_blocks, kernel_regularizer=tf.keras.regularizers.L2(0.0001), use_bias=True))\n",
        "          self.FC.append(tf.keras.layers.ReLU())          \n",
        "        self.gamma = tf.keras.layers.Dense(self.ngf * self.n_blocks, kernel_regularizer=tf.keras.regularizers.L2(0.0001),use_bias=True)  \n",
        "        self.beta = tf.keras.layers.Dense(self.ngf * self.n_blocks, kernel_regularizer=tf.keras.regularizers.L2(0.0001), use_bias=True)  \n",
        "\n",
        "        # Up-Sampling Bottleneck\n",
        "        self.UpBlock1 = []\n",
        "        for _ in range(self.n_blocks):\n",
        "          self.UpBlock1.append(ResnetAdaILNBlock(self.ngf * mult, use_bias=True, smoothing=True)) # g\n",
        "\n",
        "        # Up-Sampling \n",
        "        self.UpBlock2 = []\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2**(n_downsampling - i)\n",
        "            self.UpBlock2.append(tf.keras.layers.UpSampling2D(size=(2, 2),interpolation='nearest'))\n",
        "            self.UpBlock2.append(Conv(filters = self.ngf * mult // 2, kernel_size = 3, strides = 1, pad = 1, normal = 'ILN', act = 'relu', \n",
        "                              use_bias = False, pad_type='REFLECT', ILN_factor = self.ngf * mult // 2))\n",
        "        self.UpBlock2.append(Conv(filters = self.output_nc, kernel_size = 7, strides = 1, pad = 3, normal = None, \n",
        "                          act = 'tanh', use_bias = False, pad_type='REFLECT'))\n",
        "\n",
        "    def call(self, x):\n",
        "\n",
        "      for layer in self.DownBlock:\n",
        "        x = layer(x)\n",
        "\n",
        "      gap = self.gap(x)  # Global Average Pooling  # eg. 1 x 32 x 32 x 256 -> 1 x 256 \n",
        "      gap_logit = self.cam_fc(gap) # 1 x 1\n",
        "      gap_weight = tf.squeeze(self.cam_fc.trainable_variables[0] + self.cam_fc.trainable_variables[1])\n",
        "      gap = x * gap_weight \n",
        "      gmp = self.gmp(x)  # Global Average Pooling  # 4 x 32 x 32 x 256 -> 4 x 256 or 4 x 1 x 1 x 256?\n",
        "      gmp_logit = self.cam_fc(gmp) # 4 x 1\n",
        "      gmp_weight = tf.squeeze(self.cam_fc.trainable_variables[0] + self.cam_fc.trainable_variables[1])\n",
        "      gmp = x * gmp_weight \n",
        "\n",
        "      cam_logit = tf.keras.layers.concatenate([gap_logit, gmp_logit], axis=-1)    \n",
        "      x = tf.keras.layers.concatenate([gap, gmp], axis=-1)\n",
        "      x = self.relu(self.conv1x1(x))\n",
        "      heatmap = tf.reduce_sum(x, axis=-1, keepdims=True) ############ squeeze??\n",
        "\n",
        "      if self.light:\n",
        "        x_ = self.gap(x)\n",
        "        x_ = tf.keras.layers.Flatten()(x_)\n",
        "        for layer in self.FC:\n",
        "          x_ = layer(x_)\n",
        "      else:\n",
        "        x_ = tf.keras.layers.Flatten()(x)\n",
        "        for layer in self.FC:\n",
        "          x_ = layer(x_)\n",
        "        gamma, beta = self.gamma(x_), self.beta(x_)\n",
        "     \n",
        "      for layer in self.UpBlock1:\n",
        "        x = layer(x, gamma, beta)\n",
        "      \n",
        "      for layer in self.UpBlock2:\n",
        "        x = layer(x)\n",
        "\n",
        "      return x, cam_logit, heatmap \n",
        "\n",
        "\n",
        "\n",
        "class Conv(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, strides, pad, normal=None, act=None, use_bias=False, pad_type='REFLECT', **kwargs):\n",
        "        super(Conv, self).__init__()\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.strides = strides\n",
        "        self.pad = pad\n",
        "        self.normal = normal\n",
        "        self.act = act\n",
        "        self.use_bias = use_bias\n",
        "        self.pad_type = pad_type\n",
        "        if 'ILN_factor' in kwargs:\n",
        "            self.ILN_factor = kwargs['ILN_factor']\n",
        "\n",
        "        # normalization\n",
        "        if self.normal == 'IN':\n",
        "            self.normal = tfa.layers.InstanceNormalization(axis=3, center=True, scale=True,\n",
        "                                                           beta_initializer=\"random_uniform\",\n",
        "                                                           gamma_initializer=\"random_uniform\")\n",
        "        elif self.normal == 'ILN':\n",
        "            self.normal = ILN(self.ILN_factor)\n",
        "            \n",
        "        elif self.normal == 'SN':\n",
        "            self.conv_sn = SpectralNormalization(tf.keras.layers.Conv2D(filters=self.filters, kernel_size=(self.kernel_size, self.kernel_size),\n",
        "                                              kernel_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02),\n",
        "                                              kernel_regularizer=tf.keras.regularizers.l2(0.0001),\n",
        "                                              strides=(self.strides, self.strides), use_bias=self.use_bias))\n",
        "          \n",
        "        # activation\n",
        "        if self.act == 'relu':\n",
        "            self.act = tf.keras.layers.ReLU()\n",
        "        elif self.act == 'tanh':\n",
        "            self.act = tf.keras.activations.tanh\n",
        "        elif self.act == 'lrelu':\n",
        "            self.act = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
        "\n",
        "        # padding\n",
        "        if self.pad > 0:\n",
        "            if (self.kernel_size - self.strides) % 2 == 0:\n",
        "                self.pad_top = self.pad\n",
        "                self.pad_bottom = self.pad\n",
        "                self.pad_left = self.pad\n",
        "                self.pad_right = self.pad\n",
        "\n",
        "            else:\n",
        "                self.pad_top = self.pad\n",
        "                self.pad_bottom = self.kernel_size - self.strides - self.pad_top\n",
        "                self.pad_left = self.pad\n",
        "                self.pad_right = self.kernel_size - self.strides - self.pad_left\n",
        "\n",
        "        # conv2d\n",
        "        if self.normal != 'SN':\n",
        "            self.conv = tf.keras.layers.Conv2D(filters=self.filters, kernel_size=(self.kernel_size, self.kernel_size),\n",
        "                                              kernel_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02),\n",
        "                                              kernel_regularizer=tf.keras.regularizers.l2(0.0001),\n",
        "                                              strides=(self.strides, self.strides), use_bias=self.use_bias)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if self.pad > 0:\n",
        "            x = tf.pad(x, [[0, 0], [self.pad_top, self.pad_bottom], [self.pad_left, self.pad_right], [0, 0]], mode=self.pad_type)\n",
        "        if self.normal in ('IN','ILN'):\n",
        "            x = self.normal(self.conv(x))\n",
        "        elif self.normal == 'SN':\n",
        "            x = self.conv_sn(x)\n",
        "        else:\n",
        "            x = self.conv(x)\n",
        "        if self.act is not None:\n",
        "            x = self.act(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetBlock(tf.keras.layers.Layer): \n",
        "  def __init__(self, dim, use_bias):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.use_bias = use_bias\n",
        "        self.dim = dim\n",
        "        self.conv1 = Conv(filters = self.dim, kernel_size = 3, strides = 1, pad = 1, normal = 'IN', \n",
        "                          act = 'relu', use_bias=self.use_bias, pad_type='REFLECT')\n",
        "        self.conv2 = Conv(filters = self.dim, kernel_size = 3, strides = 1, pad = 1, normal = 'IN', \n",
        "                          act = None, use_bias=self.use_bias, pad_type='REFLECT')\n",
        "\n",
        "  def call(self, inputs):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.conv2(x)\n",
        "        x += inputs\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class ResnetAdaILNBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, dim, use_bias, smoothing=True):\n",
        "        super(ResnetAdaILNBlock, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.use_bias = use_bias\n",
        "        self.smoothing = smoothing\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters=self.dim, kernel_size=(3, 3),\n",
        "                                            kernel_regularizer=tf.keras.regularizers.l2(0.0001),\n",
        "                                            strides=(1, 1), use_bias=self.use_bias)\n",
        "        self.norm1 = adaILN(self.dim, self.smoothing)\n",
        "        self.relu1 = tf.keras.layers.ReLU()\n",
        "        self.conv2 = tf.keras.layers.Conv2D(filters=self.dim, kernel_size=(3, 3),\n",
        "                                            kernel_regularizer=tf.keras.regularizers.l2(0.0001), \n",
        "                                            strides=(1, 1), use_bias=self.use_bias)\n",
        "        self.norm2 = adaILN(self.dim, self.smoothing)\n",
        "\n",
        "    def call(self, inputs, gamma, beta):\n",
        "        x = tf.pad(inputs, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT')\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x, gamma, beta)\n",
        "        x = self.relu1(x)\n",
        "        x = tf.pad(x, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT')\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x, gamma, beta)\n",
        "        x += inputs\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class adaILN(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_features, smoothing=True):\n",
        "        super(adaILN, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = 1e-12\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def build(self, input_shape):  # 반드시 input_shape를 써야하는지? input shape에 상관 없으므로, 그냥 __init__에 넣어도 될 듯하다.\n",
        "        self.rho = tf.Variable(initial_value=tf.fill([self.num_features], 1.0), dtype=tf.float32, trainable=True, constraint=lambda x: tf.clip_by_value(x, clip_value_min=0.0, clip_value_max=1.0))\n",
        "\n",
        "    def call(self, inputs, gamma, beta):\n",
        "        in_mean, in_var = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True), tf.math.reduce_variance(inputs, axis=[1, 2], keepdims=True)\n",
        "        out_in = (inputs - in_mean) / tf.math.sqrt(in_var + self.eps)\n",
        "        ln_mean, ln_var = tf.reduce_mean(inputs, axis=[1, 2, 3], keepdims=True), tf.math.reduce_variance(inputs, axis=[1, 2, 3], keepdims=True)\n",
        "        out_ln = (inputs - ln_mean) / tf.math.sqrt(ln_var + self.eps)\n",
        "        if self.smoothing :\n",
        "            self.rho.assign(tf.clip_by_value(self.rho - tf.constant(0.1), 0.0, 1.0))\n",
        "        out = self.rho * out_in + (1-self.rho) * out_ln\n",
        "        out = out * tf.expand_dims(tf.expand_dims(gamma, axis=1), axis=1) + tf.expand_dims(tf.expand_dims(beta, axis=1), axis=1)  # (batch,channel -> batch, height, width, channel)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ILN(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_features):\n",
        "    super(ILN, self).__init__()\n",
        "    self.num_features = num_features \n",
        "    self.eps = 1e-12\n",
        "\n",
        "  def build(self,input_shape):\n",
        "    self.rho = tf.Variable(initial_value=tf.fill([self.num_features], 0.0), dtype=tf.float32, trainable=True, constraint=lambda x: tf.clip_by_value(x, clip_value_min=0.0, clip_value_max=1.0))\n",
        "    self.gamma = tf.Variable(initial_value=tf.fill([self.num_features], 1.0), dtype=tf.float32, trainable=True)\n",
        "    self.beta = tf.Variable(initial_value=tf.fill([self.num_features], 0.0), dtype=tf.float32, trainable=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    in_mean, in_var = tf.reduce_mean(inputs, axis=[1,2], keepdims=True), tf.math.reduce_variance(inputs, axis=[1,2], keepdims=True)\n",
        "    out_in = (inputs - in_mean) / tf.math.sqrt(in_var + self.eps)\n",
        "    ln_mean, ln_var = tf.reduce_mean(inputs, axis=[1,2,3], keepdims=True), tf.math.reduce_variance(inputs, axis=[1,2,3], keepdims=True)\n",
        "    out_ln = (inputs - ln_mean) / tf.math.sqrt(ln_var + self.eps)\n",
        "    out = self.rho * out_in + (1-self.rho) * out_ln\n",
        "    out = out * self.gamma + self.beta # 여기서는 바로 broadcasting가능\n",
        "    return out   \n",
        "\n",
        "\n",
        "class Discriminator_global(tf.keras.Model):\n",
        "    def __init__(self, ndf=64, n_layers=6):\n",
        "        assert(n_layers >= 0)\n",
        "        super(Discriminator_global, self).__init__()\n",
        "        self.ndf = ndf\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.model=tf.keras.Sequential()\n",
        "        self.model.add(Conv(filters = self.ndf, kernel_size = 4, strides = 2, pad = 1, normal = 'SN', act = 'lrelu', \n",
        "                       use_bias=True, pad_type='REFLECT'))\n",
        "        \n",
        "        for i in range(1, self.n_layers - 1):\n",
        "          mult = 2 ** (i - 1)\n",
        "          self.model.add(Conv(filters = self.ndf * mult * 2, kernel_size=4, strides=2, pad=1, normal = 'SN', act = 'lrelu', \n",
        "                        use_bias=True, pad_type='REFLECT'))\n",
        "          \n",
        "        mult = 2 ** (self.n_layers - 1 - 1)\n",
        "        self.model.add(Conv(filters = self.ndf * mult * 2, kernel_size=4, strides=1, pad=1, normal = 'SN', act = 'lrelu', \n",
        "                        use_bias=True, pad_type='REFLECT'))\n",
        "        \n",
        "        # Class Activation Map\n",
        "        # mult = mult * 2\n",
        "        self.gap = tf.keras.layers.GlobalAveragePooling2D()\n",
        "        self.cam_fc = SpectralNormalization(tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.L2(0.0001), kernel_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02), use_bias=True))\n",
        "        # self.gap_fc = tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.L2(0.0001), use_bias=False)\n",
        "        self.gmp = tf.keras.layers.GlobalMaxPool2D()\n",
        "        # self.gmp_fc = tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.L2(0.0001), use_bias=False)\n",
        "        # self.gmp_fc = SpectralNormalization(tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.L2(0.0001), use_bias=True))\n",
        "        # self.gmp_fc = SpectralNormalization(tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.L2(0.0001), use_bias=False))\n",
        "        self.conv1x1 = tf.keras.layers.Conv2D(filters= self.ndf * mult * 2 , kernel_size=(1,1), kernel_regularizer=tf.keras.regularizers.L2(0.0001), strides=(1,1), use_bias=True)\n",
        "        self.lrelu = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
        "        self.conv = Conv(filters = 1, kernel_size=4, strides=1, pad=1, normal = 'SN', act = None, \n",
        "                        use_bias=True, pad_type='REFLECT')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.model(inputs)\n",
        "\n",
        "        gap = self.gap(x)\n",
        "        gap_logit = self.cam_fc(gap) # 4 x 1\n",
        "        gap_weight = tf.squeeze(self.cam_fc.trainable_variables[0]+self.cam_fc.trainable_variables[1])\n",
        "        gap = x * gap_weight # feature map에 gap_weight를 element-wise로 multiply. broadcasting 가능. 즉, 4 x 32 x 32 x 256 이니까.. 4 x 1 x 1 x 256 로 multiply하면 broadcasting (test필요)   \n",
        "\n",
        "        gmp = self.gmp(x)  # Global Average Pooling  # 4 x 32 x 32 x 256 -> 4 x 256 or 4 x 1 x 1 x 256?\n",
        "        gmp_logit = self.cam_fc(gmp) # 4 x 1\n",
        "        gmp_weight = tf.squeeze(self.cam_fc.trainable_variables[0]+self.cam_fc.trainable_variables[1])\n",
        "        gmp = x * gmp_weight # feature map에 gap_weight를 element-wise로 multiply. broadcasting 가능. 즉, 4 x 32 x 32 x 256 이니까.. 4 x 1 x 1 x 256 로 multiply하면 broadcasting (test필요)   \n",
        "\n",
        "        cam_logit = tf.keras.layers.concatenate([gap_logit, gmp_logit], axis=-1) \n",
        "        x = tf.keras.layers.concatenate([gap, gmp], axis=-1)\n",
        "        x = self.lrelu(self.conv1x1(x)) \n",
        "        heatmap = tf.reduce_sum(x, axis=3, keepdims=True)\n",
        "\n",
        "        out = self.conv(x)\n",
        " \n",
        "        return out, cam_logit, heatmap\n",
        "\n",
        "\n",
        "\n",
        "class Discriminator_local(tf.keras.Model):\n",
        "    def __init__(self, ndf=64, n_layers=6):\n",
        "        assert(n_layers >= 0)\n",
        "        super(Discriminator_local, self).__init__()\n",
        "        self.ndf = ndf\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.model=tf.keras.Sequential()\n",
        "        self.model.add(Conv(filters = self.ndf, kernel_size = 4, strides = 2, pad = 1, normal = 'SN', act = 'lrelu', \n",
        "                       use_bias=True, pad_type='REFLECT'))\n",
        "        \n",
        "        for i in range(1, self.n_layers - 2 - 1):\n",
        "          mult = 2 ** (i - 1)\n",
        "          self.model.add(Conv(filters = self.ndf * mult * 2, kernel_size=4, strides=2, pad=1, normal = 'SN', act = 'lrelu', \n",
        "                        use_bias=True, pad_type='REFLECT'))\n",
        "          \n",
        "        mult = 2 ** (self.n_layers - 1 - 1)\n",
        "        self.model.add(Conv(filters = self.ndf * mult * 2, kernel_size=4, strides=1, pad=1, normal = 'SN', act = 'lrelu', \n",
        "                        use_bias=True, pad_type='REFLECT'))\n",
        "        \n",
        "        # Class Activation Map\n",
        "        # mult = mult * 2\n",
        "        self.gap = tf.keras.layers.GlobalAveragePooling2D()\n",
        "        self.cam_fc = SpectralNormalization(tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.L2(0.0001), kernel_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02), use_bias=True))\n",
        "        # self.gap_fc = tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.L2(0.0001), use_bias=False)\n",
        "        self.gmp = tf.keras.layers.GlobalMaxPool2D()\n",
        "        # self.gmp_fc = tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.L2(0.0001), use_bias=False)\n",
        "        # self.gmp_fc = SpectralNormalization(tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.L2(0.0001), use_bias=True))\n",
        "        # self.gmp_fc = SpectralNormalization(tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.L2(0.0001), use_bias=False))\n",
        "        self.conv1x1 = tf.keras.layers.Conv2D(filters= self.ndf * mult * 2 , kernel_size=(1,1), kernel_regularizer=tf.keras.regularizers.L2(0.0001), strides=(1,1), use_bias=True)\n",
        "        self.lrelu = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
        "        self.conv = Conv(filters = 1, kernel_size=4, strides=1, pad=1, normal = 'SN', act = None, \n",
        "                        use_bias=True, pad_type='REFLECT')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.model(inputs)\n",
        "\n",
        "        gap = self.gap(x)\n",
        "        gap_logit = self.cam_fc(gap) # 4 x 1\n",
        "        gap_weight = tf.squeeze(self.cam_fc.trainable_variables[0]+self.cam_fc.trainable_variables[1])\n",
        "        gap = x * gap_weight # feature map에 gap_weight를 element-wise로 multiply. broadcasting 가능. 즉, 4 x 32 x 32 x 256 이니까.. 4 x 1 x 1 x 256 로 multiply하면 broadcasting (test필요)   \n",
        "\n",
        "        gmp = self.gmp(x)  # Global Average Pooling  # 4 x 32 x 32 x 256 -> 4 x 256 or 4 x 1 x 1 x 256?\n",
        "        gmp_logit = self.cam_fc(gmp) # 4 x 1\n",
        "        gmp_weight = tf.squeeze(self.cam_fc.trainable_variables[0]+self.cam_fc.trainable_variables[1])\n",
        "        gmp = x * gmp_weight # feature map에 gap_weight를 element-wise로 multiply. broadcasting 가능. 즉, 4 x 32 x 32 x 256 이니까.. 4 x 1 x 1 x 256 로 multiply하면 broadcasting (test필요)   \n",
        "\n",
        "        cam_logit = tf.keras.layers.concatenate([gap_logit, gmp_logit], axis=-1) \n",
        "        x = tf.keras.layers.concatenate([gap, gmp], axis=-1)\n",
        "        x = self.lrelu(self.conv1x1(x)) \n",
        "        heatmap = tf.reduce_sum(x, axis=3, keepdims=True)\n",
        "\n",
        "        out = self.conv(x)\n",
        " \n",
        "        return out, cam_logit, heatmap\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-tbAJvcfAMC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "9181ce28-dff4-40b2-96ca-05f993c5a4cf"
      },
      "source": [
        "import time \n",
        "from glob import glob\n",
        "import tensorflow_datasets as tfds\n",
        "import datetime\n",
        "import os\n",
        "import numpy as np \n",
        "from matplotlib.pyplot import imsave\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "def ad_loss(y_pred, y_true):\n",
        "    return tf.reduce_mean(tf.math.squared_difference(y_pred, y_true))\n",
        "\n",
        "def bce_loss(y_pred, y_true):\n",
        "    return tf.keras.losses.BinaryCrossentropy(from_logits=True)(y_true, y_pred)\n",
        "\n",
        "def id_loss(y_pred, y_true):\n",
        "    return tf.reduce_mean(tf.abs(y_pred - y_true))\n",
        "\n",
        "def recon_loss(y_pred, y_true):\n",
        "    return tf.reduce_mean(tf.abs(y_pred - y_true))\n",
        "\n",
        "def normalize(image):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image = (image / 127.5) - 1 # [-1,1]\n",
        "  return image\n",
        "\n",
        "def train_augment(image,label,img_size):\n",
        "  image = tf.image.resize(image, [img_size+30, img_size+30], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "  image = tf.image.random_crop(image, size=[img_size, img_size, 3])\n",
        "  image = tf.image.random_flip_left_right(image)\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image = normalize(image)\n",
        "  return image\n",
        "\n",
        "def test_augment(image,label,img_size):\n",
        "  image = tf.image.resize(image, [img_size, img_size])\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image = normalize(image)\n",
        "  return image\n",
        "\n",
        "\n",
        "def image_save(real_images, fake_images, path, iter):\n",
        "  assert real_images.shape == fake_images.shape\n",
        "  real_images = real_images * 0.5 + 0.5  # [-1,1 -> 0,1]\n",
        "  fake_images = fake_images * 0.5 + 0.5  \n",
        "  batch_size, h, w, c = real_images.shape\n",
        "  figure = np.zeros((batch_size * h ,w *2, c))\n",
        "  idx = 0\n",
        "  for real_img, fake_img in zip(real_images, fake_images):\n",
        "    figure[h*idx:h*(idx+1), 0:w, ...] = real_img # loc [0-255, 0-255, ...]\n",
        "    figure[h*idx:h*(idx+1), w: , ...] = fake_img # loc [0-255, 256-, ...]\n",
        "    idx += 1  \n",
        "  suffix = '.png'\n",
        "  path = os.path.join(path, 'iter_' + str(iter) + suffix)\n",
        "  plt.imsave(path ,figure)\n",
        "\n",
        "\n",
        "class UGATIT(object) :\n",
        "    def __init__(self):\n",
        "\n",
        "        # if self.light :\n",
        "        #     self.model_name = 'UGATIT_light'\n",
        "        # else :\n",
        "        #     self.model_name = 'UGATIT'\n",
        "\n",
        "        # self.result_dir = args.result_dir\n",
        "        # self.ckpt_dir = args.ckpt_dir\n",
        "        # self.dataset = args.dataset\n",
        "\n",
        "        # self.iterations = args.iterations\n",
        "        # self.decay_flag = args.decay_flag\n",
        "\n",
        "        # self.batch_size = args.batch_size\n",
        "        # self.print_freq = args.print_freq\n",
        "        # self.save_freq = args.save_freq\n",
        "\n",
        "        # self.lr = args.lr\n",
        "        # self.weight_decay = args.weight_decay\n",
        "        # self.ch = args.ch\n",
        "\n",
        "        # \"\"\" Weight \"\"\"\n",
        "        # self.adv_weight = args.adv_weight\n",
        "        # self.cycle_weight = args.cycle_weight\n",
        "        # self.identity_weight = args.identity_weight\n",
        "        # self.cam_weight = args.cam_weight\n",
        "\n",
        "        # \"\"\" Generator \"\"\"\n",
        "        # self.n_res = args.n_res\n",
        "\n",
        "        # \"\"\" Discriminator \"\"\"\n",
        "        # self.n_dis = args.n_dis\n",
        "\n",
        "        # self.img_size = args.img_size\n",
        "        # self.img_ch = args.img_ch\n",
        "\n",
        "        # self.resume = args.resume\n",
        "\n",
        "        self.light = False\n",
        "        self.model_name = 'UGATIT'\n",
        "\n",
        "        self.ckpt_path = 'ckpt'\n",
        "        self.tensorboard_path = 'tensorboard'\n",
        "        self.img_save_path = 'img_save'\n",
        "        self.dataset = 'horse2zebra'\n",
        "\n",
        "        self.iterations = 100000\n",
        "        self.batch_size = 1\n",
        "        self.sample_num = 5\n",
        "        self.print_freq = 1\n",
        "        self.save_freq = 10000\n",
        "        self.sample_freq = 1\n",
        "        self.lr = 0.0001\n",
        "        self.weight_decay = 0.0001\n",
        "        self.ch = 64\n",
        "\n",
        "        \"\"\" Weight \"\"\"\n",
        "        self.adv_weight = 1\n",
        "        self.cycle_weight = 10\n",
        "        self.identity_weight = 10\n",
        "        self.cam_weight = 1000\n",
        "\n",
        "        \"\"\" Generator \"\"\"\n",
        "        self.n_res = 4\n",
        "\n",
        "        \"\"\" Discriminator \"\"\"\n",
        "        self.n_dis = 6\n",
        "\n",
        "        self.img_size = 128\n",
        "        self.img_ch = 3\n",
        "\n",
        "        print()\n",
        "\n",
        "        print(\"##### Information #####\")\n",
        "        print(\"# light : \", self.light)\n",
        "        # print(\"# dataset : \", self.dataset)\n",
        "        print(\"# batch_size : \", self.batch_size)\n",
        "        print(\"# iteration per epoch : \", self.iterations)\n",
        "\n",
        "        print()\n",
        "\n",
        "        print(\"##### Generator #####\")\n",
        "        print(\"# residual blocks : \", self.n_res)\n",
        "\n",
        "        print()\n",
        "\n",
        "        print(\"##### Discriminator #####\")\n",
        "        print(\"# discriminator layer : \", self.n_dis)\n",
        "\n",
        "        print()\n",
        "\n",
        "        print(\"##### Weight #####\")\n",
        "        print(\"# adv_weight : \", self.adv_weight)\n",
        "        print(\"# cycle_weight : \", self.cycle_weight)\n",
        "        print(\"# identity_weight : \", self.identity_weight)\n",
        "        print(\"# cam_weight : \", self.cam_weight)\n",
        "\n",
        "    ##################################################################################\n",
        "    # Model\n",
        "    ##################################################################################\n",
        "\n",
        "    def build(self):\n",
        "\n",
        "    # Dataset\n",
        "          \n",
        "        if self.dataset != 'horse2zebra':            \n",
        "            train_A = tf.data.Dataset.list_files(f\"./{self.dataset}/trainA/*.jpg\")\n",
        "            train_A = train_A.map(lambda x:train_augment(x, self.img_size), num_parallel_calls=AUTOTUNE)\n",
        "            train_A = train_A.repeat()\n",
        "            train_A = train_A.batch(self.batch_size)\n",
        "            train_A = train_A.prefetch(AUTOTUNE)\n",
        "            train_B = tf.data.Dataset.list_files(f\"./{self.dataset}/trainB/*.jpg\")\n",
        "            train_B = train_B.map(lambda x: train_augment(x, self.img_size), num_parallel_calls=AUTOTUNE)\n",
        "            train_B = train_B.repeat()\n",
        "            train_B = train_B.batch(self.batch_size)\n",
        "            train_B = train_B.prefetch(AUTOTUNE)\n",
        "            train_dataset = tf.data.Dataset.zip((train_A, train_B))\n",
        "            self.train_iterator = iter(train_dataset)\n",
        "            test_A = tf.data.Dataset.list_files(f\"./{self.dataset}/testA/*.jpg\")\n",
        "            test_A = test_A.map(lambda x: test_augment(x, self.img_size), num_parallel_calls=AUTOTUNE)\n",
        "            test_A = test_A.repeat()\n",
        "            test_A = test_A.batch(self.sample_num)\n",
        "            test_A = test_A.prefetch(AUTOTUNE)            \n",
        "            self.test_iterator = iter(test_A) # only Selfie -> Anime\n",
        "\n",
        "                    \n",
        "        elif self.dataset == 'horse2zebra':\n",
        "            dataset, _ = tfds.load('cycle_gan/horse2zebra',\n",
        "                                    with_info=True, as_supervised=True)\n",
        "            train_A, train_B = dataset['trainA'], dataset['trainB']\n",
        "            test_A, test_B = dataset['testA'], dataset['testB']\n",
        "            train_A = train_A.map(lambda x, y: train_augment(x, y, self.img_size), num_parallel_calls=AUTOTUNE)\n",
        "            train_A = train_A.repeat()\n",
        "            train_A = train_A.batch(self.batch_size)\n",
        "            train_A = train_A.prefetch(AUTOTUNE)\n",
        "            train_B = train_B.map(lambda x, y: train_augment(x, y, self.img_size), num_parallel_calls=AUTOTUNE)\n",
        "            train_B = train_B.repeat()\n",
        "            train_B = train_B.batch(self.batch_size)\n",
        "            train_B = train_B.prefetch(AUTOTUNE)\n",
        "            train_dataset = tf.data.Dataset.zip((train_A, train_B))\n",
        "            self.train_iterator = iter(train_dataset)\n",
        "            test_A = test_A.map(lambda x, y: test_augment(x, y, self.img_size), num_parallel_calls=AUTOTUNE)\n",
        "            test_A = test_A.batch(self.sample_num)\n",
        "            test_A = test_A.repeat()\n",
        "            self.test_iterator = iter(test_A) # only Horses -> Zebras\n",
        "\n",
        "        # Model building (Total 6 models)\n",
        "        self.genA2B =  ResnetGenerator(output_nc=3, ngf=self.ch, n_blocks=self.n_res, light=self.light) # A -> B gen\n",
        "        self.genB2A =  ResnetGenerator(output_nc=3, ngf=self.ch, n_blocks=self.n_res, light=self.light) # B -> A gen\n",
        "        self.disGA = Discriminator_global(ndf=self.ch, n_layers=self.n_dis) # Global A disc\n",
        "        self.disGB = Discriminator_global(ndf=self.ch, n_layers=self.n_dis) # Global B disc\n",
        "        self.disLA = Discriminator_local(ndf=self.ch, n_layers=self.n_dis) # Local A disc\n",
        "        self.disLB = Discriminator_local(ndf=self.ch, n_layers=self.n_dis) # Local B disc\n",
        "\n",
        "        # Optimizer\n",
        "        self.gen_opt = tf.keras.optimizers.Adam(learning_rate=self.lr, beta_1=0.5, beta_2=0.999)\n",
        "        self.disc_opt = tf.keras.optimizers.Adam(learning_rate=self.lr, beta_1=0.5, beta_2=0.999)\n",
        "\n",
        "\n",
        "        # dir\n",
        "        current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        self.ckpt_path = os.path.join(current_time, self.ckpt_path)\n",
        "        self.tensorboard_path = os.path.join(current_time, self.tensorboard_path)\n",
        "        self.img_save_path = os.path.join(current_time, self.img_save_path)\n",
        "        os.makedirs(self.tensorboard_path)\n",
        "        os.makedirs(self.img_save_path)\n",
        "\n",
        "        # ckpt manager \n",
        "        ckpt = tf.train.Checkpoint(genA2B=self.genA2B, \n",
        "                                   genB2A=self.genB2A,\n",
        "                                   disGA=self.disGA,\n",
        "                                   disGB=self.disGB,\n",
        "                                   disLA=self.disLA,\n",
        "                                   disLB=self.disLB)\n",
        "        self.ckpt_manager = tf.train.CheckpointManager(ckpt, self.ckpt_path, max_to_keep=4)\n",
        "\n",
        "        # summary writer\n",
        "        train_log_path = os.path.join(self.tensorboard_path, 'train')\n",
        "        self.train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
        "        print(f'******* Train result log will be written to {train_log_path} ******')   \n",
        "        test_log_path = os.path.join(self.tensorboard_path, 'test')\n",
        "        self.test_summary_writer = tf.summary.create_file_writer(test_log_path)\n",
        "        print(f'******* Test result log will be written to {test_log_path} ******')\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "      print('training start !')     \n",
        "\n",
        "      # for each epoch\n",
        "      for iter in range(1, self.iterations+1):\n",
        "\n",
        "          start_time = time.time()\n",
        "        \n",
        "          real_A, real_B = self.train_iterator.get_next()\n",
        "\n",
        "          # discriminator update\n",
        "          with tf.GradientTape() as disc_tape:\n",
        "\n",
        "            fake_A2B, _, _ = self.genA2B(real_A)\n",
        "            fake_B2A, _, _ = self.genB2A(real_B)\n",
        "\n",
        "            real_GA_logit, real_GA_cam_logit, _ = self.disGA(real_A)\n",
        "            real_LA_logit, real_LA_cam_logit, _ = self.disLA(real_A)\n",
        "            real_GB_logit, real_GB_cam_logit, _ = self.disGB(real_B)\n",
        "            real_LB_logit, real_LB_cam_logit, _ = self.disLB(real_B)\n",
        "\n",
        "            fake_GA_logit, fake_GA_cam_logit, _ = self.disGA(fake_B2A)\n",
        "            fake_LA_logit, fake_LA_cam_logit, _ = self.disLA(fake_B2A)\n",
        "            fake_GB_logit, fake_GB_cam_logit, _ = self.disGB(fake_A2B)\n",
        "            fake_LB_logit, fake_LB_cam_logit, _ = self.disLB(fake_A2B)\n",
        "\n",
        "            D_ad_loss_GA = ad_loss(real_GA_logit,tf.ones_like(real_GA_logit, dtype=tf.float32)) + ad_loss(fake_GA_logit,tf.zeros_like(fake_GA_logit, dtype=tf.float32))\n",
        "            D_ad_cam_loss_GA = ad_loss(real_GA_cam_logit,tf.ones_like(real_GA_cam_logit, dtype=tf.float32)) + ad_loss(fake_GA_cam_logit, tf.zeros_like(fake_GA_cam_logit, dtype=tf.float32))\n",
        "            D_ad_loss_LA = ad_loss(real_LA_logit,tf.ones_like(real_LA_logit, dtype=tf.float32)) + ad_loss(fake_LA_logit, tf.zeros_like(fake_LA_logit, dtype=tf.float32))\n",
        "            D_ad_cam_loss_LA = ad_loss(real_LA_cam_logit,tf.ones_like(real_LA_cam_logit, dtype=tf.float32)) + ad_loss(fake_LA_cam_logit, tf.zeros_like(fake_LA_cam_logit, dtype=tf.float32))\n",
        "            D_ad_loss_GB = ad_loss(real_GB_logit,tf.ones_like(real_GB_logit, dtype=tf.float32)) + ad_loss(fake_GB_logit,tf.zeros_like(fake_GB_logit, dtype=tf.float32))\n",
        "            D_ad_cam_loss_GB = ad_loss(real_GB_cam_logit,tf.ones_like(real_GB_cam_logit, dtype=tf.float32)) + ad_loss(fake_GB_cam_logit, tf.zeros_like(fake_GB_cam_logit, dtype=tf.float32))\n",
        "            D_ad_loss_LB = ad_loss(real_LB_logit,tf.ones_like(real_LB_logit, dtype=tf.float32)) + ad_loss(fake_LB_logit, tf.zeros_like(fake_LB_logit, dtype=tf.float32))\n",
        "            D_ad_cam_loss_LB = ad_loss(real_LB_cam_logit,tf.ones_like(real_LB_cam_logit, dtype=tf.float32)) + ad_loss(fake_LB_cam_logit, tf.zeros_like(fake_LB_cam_logit, dtype=tf.float32))\n",
        "            D_loss_A = self.adv_weight * (D_ad_loss_GA + D_ad_cam_loss_GA + D_ad_loss_LA + D_ad_cam_loss_LA)\n",
        "            D_loss_B = self.adv_weight * (D_ad_loss_GB + D_ad_cam_loss_GB + D_ad_loss_LB + D_ad_cam_loss_LB)\n",
        "            D_reg_loss = tf.reduce_sum(self.disGA.losses) + tf.reduce_sum(self.disLA.losses) + tf.reduce_sum(self.disGB.losses) + tf.reduce_sum(self.disLB.losses)\n",
        "            D_loss = D_loss_A + D_loss_B + D_reg_loss\n",
        "\n",
        "            if iter == 1:            \n",
        "              self.disc_tot_vars = []\n",
        "              for disc_model in [self.disGA, self.disGB, self.disLA, self.disLB]:\n",
        "                self.disc_tot_vars.extend(disc_model.trainable_variables)\n",
        "          \n",
        "          disc_grad = disc_tape.gradient(D_loss, self.disc_tot_vars)\n",
        "          self.disc_opt.apply_gradients(zip(disc_grad, self.disc_tot_vars))\n",
        "\n",
        "          # generator update\n",
        "          with tf.GradientTape() as gen_tape:\n",
        "\n",
        "            fake_A2B, fake_A2B_cam_logit, _ = self.genA2B(real_A)\n",
        "            fake_B2A, fake_B2A_cam_logit, _ = self.genB2A(real_B)\n",
        "\n",
        "            fake_A2B2A, _, _ = self.genB2A(fake_A2B)\n",
        "            fake_B2A2B, _, _ = self.genA2B(fake_B2A)\n",
        "\n",
        "            fake_A2A, fake_A2A_cam_logit, _ = self.genB2A(real_A)\n",
        "            fake_B2B, fake_B2B_cam_logit, _ = self.genA2B(real_B)\n",
        "\n",
        "            fake_GA_logit, fake_GA_cam_logit, _ = self.disGA(fake_B2A)\n",
        "            fake_LA_logit, fake_LA_cam_logit, _ = self.disLA(fake_B2A)\n",
        "            fake_GB_logit, fake_GB_cam_logit, _ = self.disGB(fake_A2B)\n",
        "            fake_LB_logit, fake_LB_cam_logit, _ = self.disLB(fake_A2B)\n",
        "\n",
        "            G_ad_loss_GA = ad_loss(fake_GA_logit,tf.ones_like(fake_GA_logit, dtype=tf.float32))\n",
        "            G_ad_cam_loss_GA = ad_loss(fake_GA_cam_logit, tf.ones_like(fake_GA_cam_logit, dtype=tf.float32))\n",
        "            G_ad_loss_LA = ad_loss(fake_LA_logit, tf.ones_like(fake_LA_logit, dtype=tf.float32))\n",
        "            G_ad_cam_loss_LA = ad_loss(fake_LA_cam_logit, tf.ones_like(fake_LA_cam_logit, dtype=tf.float32))\n",
        "            G_ad_loss_GB = ad_loss(fake_GB_logit,tf.ones_like(fake_GB_logit, dtype=tf.float32))\n",
        "            G_ad_cam_loss_GB = ad_loss(fake_GB_cam_logit, tf.ones_like(fake_GB_cam_logit, dtype=tf.float32))\n",
        "            G_ad_loss_LB = ad_loss(fake_LB_logit, tf.ones_like(fake_LB_logit, dtype=tf.float32))\n",
        "            G_ad_cam_loss_LB = ad_loss(fake_LB_cam_logit, tf.ones_like(fake_LB_cam_logit, dtype=tf.float32))\n",
        "\n",
        "            G_recon_loss_A = recon_loss(fake_A2B2A, real_A)\n",
        "            G_recon_loss_B = recon_loss(fake_B2A2B, real_B)\n",
        "\n",
        "            G_identity_loss_A = id_loss(fake_A2A, real_A)\n",
        "            G_identity_loss_B = id_loss(fake_B2B, real_B)\n",
        "\n",
        "            G_cam_loss_A = bce_loss(fake_B2A_cam_logit, tf.ones_like(fake_B2A_cam_logit)) + bce_loss(fake_A2A_cam_logit, tf.zeros_like(fake_A2A_cam_logit))\n",
        "            G_cam_loss_B = bce_loss(fake_A2B_cam_logit, tf.ones_like(fake_A2B_cam_logit)) + bce_loss(fake_B2B_cam_logit, tf.zeros_like(fake_B2B_cam_logit))\n",
        "            \n",
        "            G_loss_A = self.adv_weight * (G_ad_loss_GA + G_ad_cam_loss_GA + G_ad_loss_LA + G_ad_cam_loss_LA) + self.cycle_weight * G_recon_loss_A + self.identity_weight * G_identity_loss_A + self.cam_weight * G_cam_loss_A\n",
        "            G_loss_B = self.adv_weight * (G_ad_loss_GB + G_ad_cam_loss_GB + G_ad_loss_LB + G_ad_cam_loss_LB) + self.cycle_weight * G_recon_loss_B + self.identity_weight * G_identity_loss_B + self.cam_weight * G_cam_loss_B\n",
        "            G_reg_loss = tf.reduce_sum(self.genA2B.losses) + tf.reduce_sum(self.genB2A.losses)\n",
        "            G_loss = G_loss_A + G_loss_B + G_reg_loss\n",
        "\n",
        "            if iter == 1:\n",
        "              self.gen_tot_vars= []            \n",
        "              self.gen_tot_vars = self.genA2B.trainable_variables + self.genB2A.trainable_variables\n",
        "\n",
        "          gen_grad = gen_tape.gradient(G_loss, self.gen_tot_vars)\n",
        "          self.gen_opt.apply_gradients(zip(gen_grad, self.gen_tot_vars))\n",
        "\n",
        "          with self.train_summary_writer.as_default():\n",
        "            tf.summary.scalar('Discriminator_loss', D_loss, step=iter)\n",
        "            tf.summary.scalar('Generator_loss', G_loss, step=iter)\n",
        "\n",
        "          if iter % self.print_freq == 0:            \n",
        "            print(\"[%5d/%5d] time: %4.4f d_loss: %.8f, g_loss: %.8f\" % (iter, self.iterations, time.time() - start_time, D_loss, G_loss))\n",
        "\n",
        "          if iter % self.sample_freq == 0: # Only A->B\n",
        "            real_A = self.test_iterator.get_next() \n",
        "            fake_A2B, _, _ = self.genA2B(real_A)            \n",
        "            image_save(real_A, fake_A2B, self.img_save_path, iter)\n",
        "       \n",
        "          if iter % self.save_freq == 0:\n",
        "            self.ckpt_manager.save(checkpoint_number=iter)\n",
        "            print(f'******* {str(iter)} checkpoint saved to {self.ckpt_path} ******') \n",
        " "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-77328fcde70b>\"\u001b[0;36m, line \u001b[0;32m211\u001b[0m\n\u001b[0;31m    self.disGB = Discriminator_global(ndf=self.ch, n_layers=self.n_dis) # Global B disc\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdcX5hrypON7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.listdir()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLbE7fhKdC6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp = UGATIT()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYxXybHsdRcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp.build()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chHV-Cthp5ci",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   항목 추가\n",
        "*   항목 추가\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCp9ylaFNH9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whQ3y0YFJbmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}